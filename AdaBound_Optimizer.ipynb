{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBound \n",
    "This is the Tensorflow Implementation of the AdaBound optimizer used in all experiments.\n",
    "Its was based on the Tensorflow implementation of Adam(https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/optimizer_v2/adam.py#L32-L274). With some implementation details inspired by Github user huyu398's AdaBound implementation(https://github.com/taki0112/AdaBound-Tensorflow/blob/master/AdaBound.py). \n",
    "\n",
    "It works on dense and sparse data.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imlpementation of AdaBound from : \n",
    "# Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n",
    "# https://openreview.net/forum?id=Bkg3g2R9FX\n",
    "\n",
    "# Modified Version of the Tensorflow Adam Implementation \n",
    "# https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/keras/optimizer_v2/adam.py#L32-L274\n",
    "\n",
    "# also inspired by GitHub user huyu398's AdaBound implementation \n",
    "# https://github.com/taki0112/AdaBound-Tensorflow/blob/master/AdaBound.py\n",
    "\n",
    "from tensorflow.python.framework import ops \n",
    "from tensorflow.python.keras import backend_config \n",
    "from tensorflow.python.keras.optimizer_v2 import optimizer_v2 \n",
    "from tensorflow.python.ops import array_ops \n",
    "from tensorflow.python.ops import control_flow_ops \n",
    "from tensorflow.python.ops import math_ops \n",
    "from tensorflow.python.ops import state_ops \n",
    "from tensorflow.python.ops import resource_variable_ops\n",
    "from tensorflow.python.training import training_ops \n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "from tensorflow import clip_by_value\n",
    "\n",
    "class AdaBound(optimizer_v2.OptimizerV2):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 learning_rate=1e-3, \n",
    "                 beta_1=0.9, \n",
    "                 beta_2=0.999, \n",
    "                 final_lr=0.1, \n",
    "                 gamma=1e-3, \n",
    "                 epsilon=1e-8, \n",
    "                 amsbound=False, \n",
    "                 name='AdaBound', \n",
    "                 **kwargs):\n",
    "        super(AdaBound, self).__init__(name, **kwargs)\n",
    "        self._set_hyper('learning_rate', kwargs.get('lr',learning_rate))\n",
    "        self._set_hyper('decay', self._initial_decay)\n",
    "        self._set_hyper('beta_1', beta_1)\n",
    "        self._set_hyper('beta_2', beta_2)\n",
    "        self._set_hyper('final_lr', final_lr)\n",
    "        self._set_hyper('gamma', gamma)\n",
    "        self.epsilon = epsilon or backend_config.epsilon()\n",
    "        self.amsbound = amsbound\n",
    "        self.base_lr = learning_rate\n",
    "        \n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'm')\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, 'v')\n",
    "        if self.amsbound:\n",
    "            for var in var_list:\n",
    "                self.add_slot(var, 'vhat')\n",
    "                \n",
    "    def _prepare_local(self, var_device, var_dtype, apply_state):\n",
    "        super(AdaBound, self)._prepare_local(var_device, var_dtype, apply_state)\n",
    "        \n",
    "        local_step = math_ops.cast(self.iterations +1, var_dtype)\n",
    "        beta_1_t = array_ops.identity(self._get_hyper('beta_1', var_dtype))\n",
    "        beta_2_t = array_ops.identity(self._get_hyper('beta_2', var_dtype))\n",
    "        beta_1_power = math_ops.pow(beta_1_t, local_step)\n",
    "        beta_2_power = math_ops.pow(beta_2_t, local_step)\n",
    "        gamma_power = math_ops.pow(self._get_hyper('gamma', var_dtype), local_step)\n",
    "        lr = apply_state[(var_device, var_dtype)]['lr_t'] * \\\n",
    "              ((math_ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)))\n",
    "        final_lr = math_ops.multiply(self._get_hyper('final_lr', var_dtype),\n",
    "                    math_ops.divide(apply_state[(var_device, var_dtype)]['lr_t'], \n",
    "                                 ops.convert_to_tensor(self.base_lr, var_dtype)))\n",
    "        apply_state[(var_device, var_dtype)].update(dict(\n",
    "            lr=lr, \n",
    "            epsilon=ops.convert_to_tensor(self.epsilon, var_dtype), \n",
    "            gamma_power=gamma_power,\n",
    "            final_lr=final_lr,\n",
    "            beta_1_t=beta_1_t, \n",
    "            one_minus_beta_1_t=1 - beta_1_t, \n",
    "            beta_2_t=beta_2_t, \n",
    "            beta_2_power=beta_2_power,\n",
    "            one_minus_beta_2_t=1 - beta_2_t\n",
    "        ))\n",
    "        \n",
    "    def set_weights(self, weights):\n",
    "        params = self.weights\n",
    "        \n",
    "        num_vars = int((len(params) -1) /2)\n",
    "        if len(weights) == 3 * num_vars +1:\n",
    "            weights = weights[:len(params)]\n",
    "        super(AdaBound, self).set_weights(weights)\n",
    "        \n",
    "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "        var_device, var_dtype = var.device, var.dtype.base_dtype \n",
    "        coefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
    "                        or self._fallback_apply_state(var_device, var_dtype))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        lower_bound = coefficients['final_lr'] * (1. - 1. / (coefficients['gamma_power'] + 1.))\n",
    "        upper_bound = coefficients['final_lr'] * (1. + 1. / (coefficients['gamma_power']))\n",
    "        \n",
    "        m = self.get_slot(var, 'm')\n",
    "        m_scaled_g_values = grad * coefficients['one_minus_beta_1_t']\n",
    "        m_t = state_ops.assign(m, m*coefficients['beta_1_t'] + m_scaled_g_values, use_locking=self._use_locking)\n",
    "        \n",
    "        \n",
    "        v = self.get_slot(var, 'v')\n",
    "        v_scaled_g_values = (grad * grad) * coefficients['one_minus_beta_2_t']\n",
    "        v_t = state_ops.assign(v, v * coefficients['beta_2_t'] + v_scaled_g_values, use_locking=self._use_locking)\n",
    "        \n",
    "        if self.amsbound:\n",
    "            vhat = self.get_slot(var, 'vhat')\n",
    "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat), use_locking=self._use_locking)\n",
    "            v_sqrt = math_ops.sqrt(vhat_t)\n",
    "        else:\n",
    "            v_sqrt = math_ops.sqrt(v_t)\n",
    "        \n",
    "        step_size_bound = coefficients['lr'] / (v_sqrt + coefficients['epsilon'])\n",
    "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
    "        \n",
    "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
    "        \n",
    "        if self.amsbound:\n",
    "            return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t])\n",
    "    \n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indcs, apply_state=None):\n",
    "        var_device, var_dtype = var.device, var.dtype.base_dtype \n",
    "        coefficients = ((apply_state or {}).get((var_device, var_dtype))\n",
    "                        or self._fallback_apply_state(var_device, var_dtype))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        lower_bound = coefficients['final_lr'] * (1. - 1. / (coefficients['gamma_power'] + 1.))\n",
    "        upper_bound = coefficients['final_lr'] * (1. + 1. / (coefficients['gamma_power']))\n",
    "        \n",
    "        m = self.get_slot(var, 'm')\n",
    "        m_scaled_g_values = grad * coefficients['one_minus_beta_1_t']\n",
    "        m_t = state_ops.assign(m, m*coefficients['beta_1_t'] , use_locking=self._use_locking, name='assign_m_t')\n",
    "        with ops.control_dependencies([m_t]):\n",
    "            m_t = self._resource_scatter_add(m, indcs, m_scaled_g_values)\n",
    "        \n",
    "        \n",
    "        v = self.get_slot(var, 'v')\n",
    "        v_scaled_g_values = (grad * grad) * coefficients['one_minus_beta_2_t']\n",
    "        v_t = state_ops.assign(v, v * coefficients['beta_2_t'] , use_locking=self._use_locking, name='assign_v_t')\n",
    "        with ops.control_dependencies([v_t]):\n",
    "            v_t = self._resource_scatter_add(v, indcs, v_scaled_g_values )\n",
    "        \n",
    "        if self.amsbound:\n",
    "            vhat = self.get_slot(var, 'vhat')\n",
    "            vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat), use_locking=self._use_locking)\n",
    "            v_sqrt = math_ops.sqrt(vhat_t)\n",
    "        else:\n",
    "            v_sqrt = math_ops.sqrt(v_t)\n",
    "        \n",
    "        step_size_bound = coefficients['lr'] / (v_sqrt + coefficients['epsilon'])\n",
    "        bounded_lr = m_t * clip_by_value(step_size_bound, lower_bound, upper_bound)\n",
    "        \n",
    "        var_update = state_ops.assign_sub(var, bounded_lr, use_locking=self._use_locking)\n",
    "        \n",
    "        \n",
    "        if self.amsbound:\n",
    "            return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(AdaBound, self).get_config()\n",
    "        config.update({\n",
    "            'learning_rate' : self._serialize_hyperparameter('learning_rate'),\n",
    "            'decay': self._serialize_hyperparameter('decay'),\n",
    "            'beta_1': self._serialize_hyperparameter('beta_1'),\n",
    "            'beta_2': self._serialize_hyperparameter('beta_2'),\n",
    "            'gamma': self._serialize_hyperparameter('gamma'),\n",
    "            'final_lr': self._serialize_hyperparameter('final_lr'), \n",
    "            'epsilon': self.epsilon, \n",
    "            'amsbound': self.amsbound,\n",
    "        })\n",
    "        return config\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
